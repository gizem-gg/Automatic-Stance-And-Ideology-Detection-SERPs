{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0fea6-0c0e-44fe-8501-53c27309794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_utils():\n",
    "    # Get the GPU device name.\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    # The device name should look like the following:\n",
    "    if device_name == '/device:GPU:0':\n",
    "        print('Found GPU at: {}'.format(device_name))\n",
    "    else:\n",
    "        raise SystemError('GPU device not found')\n",
    "\n",
    "    device = None\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e392e1-5cf6-4111-b8d2-6c10aca5d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530bb8f-67da-4a60-b27f-250653a323ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model):\n",
    "    tokenizer = None\n",
    "    from transformers import AutoTokenizer, DistilBertTokenizer, BertTokenizer, RobertaTokenizer, AutoModelWithLMHead\n",
    "    #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    #tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d18cdf-6cb7-4958-9aa6-b63418c34d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concanListStrings(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" \" + list2[idx])\n",
    "    return list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04d33d-a5b1-4bb7-a66d-ab64d652391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concanListStrings_sep(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" [SEP] \" + list2[idx])\n",
    "\n",
    "    return list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33c6c1-348a-43fb-99e7-d22376cccfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f218ee8-1dd6-4acf-b555-7d81ce733315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_batches_datasets(datasetTrain, datasetVal, batch_size = 16):\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "        \n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8, drop_last=True\n",
    "        )\n",
    "\n",
    "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(\n",
    "            datasetVal, # The validation samples.\n",
    "            sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "            batch_size = batch_size, # Evaluate with this batch size.\n",
    "            num_workers=8, drop_last=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    #validation_dataloader = DataLoader(\n",
    "    #        datasetVal, # The validation samples.\n",
    "    #        sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "    #        batch_size = batch_size, # Evaluate with this batch size.\n",
    "    #        num_workers=0, drop_last=True\n",
    "    #)\n",
    "    \n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf327845-a28f-428f-84e0-578e4ccc2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(training_stats):\n",
    "    # Display floats with two decimal places.\n",
    "    pd.set_option('precision', 4)\n",
    "    \n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "    # A hack to force the column headers to wrap.\n",
    "    #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "\n",
    "    # Display the table.\n",
    "    print(df_stats)\n",
    "    return df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a96359-04fb-40b0-a6fc-7713a21f83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df_stats, last_epoch):\n",
    "    # Use plot styling from seaborn.\n",
    "    sns.set(style='darkgrid')\n",
    "\n",
    "    # Increase the plot size and font size.\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    \n",
    "    plot1 = plt.figure(1)\n",
    "    \n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training_Loss\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Val_Loss\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    #plt.autoscale(enable=True, axis='x')\n",
    "    \n",
    "    plot2 = plt.figure(2)\n",
    "\n",
    "    x_ticks = []\n",
    "    for currEpoch in range(1, last_epoch+1):\n",
    "        x_ticks.append(currEpoch)\n",
    "    #plt.xticks(x_ticks)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.plot(df_stats['Valid. Stance Accur.'], 'b-o', label=\"Valid. Stance Accur.\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Val Stance & Ideology Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Acc\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cf1f3-cc49-4201-b430-ce91fe07c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_acc_max_stance = -1\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, val_acc_stance, model_save_state, model_save_path, model, tokenizer):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc_stance, model_save_state, model_save_path, model, tokenizer)\n",
    "            self.val_acc_max_stance = val_acc_stance\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc_stance, model_save_state, model_save_path, model, tokenizer)\n",
    "            self.val_acc_max_stance = val_acc_stance\n",
    "            self.counter = 0\n",
    "\n",
    "            #self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, val_acc_stance, model_save_state, model_save_path, model, tokenizer):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            print(f'Validation acc stance : ({self.val_acc_max_stance:.6f} --> {val_acc_stance:.6f}).  Saving model ...')\n",
    "        #torch.save(model.module.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "        torch.save(model_save_state, model_save_path)\n",
    "        \n",
    "        \n",
    "        #model.save_pretrained('model_save/')\n",
    "        #tokenizer.save_pretrained('model_save/')\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        #torch.save(model, './model_save/entire_model.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fdbf90-842a-40de-b728-0515f904c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStoppingIdeology:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_acc_max_stance = -1\n",
    "        self.val_acc_max_ideology = -1\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, val_acc_stance, val_acc_ideology, model_save_state, model_save_path, model, tokenizer):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc_stance, val_acc_ideology, model_save_state, model_save_path, model, tokenizer)\n",
    "            self.val_acc_max_stance = val_acc_stance\n",
    "            self.val_acc_max_ideology = val_acc_ideology\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, val_acc_stance, val_acc_ideology, model_save_state, model_save_path, model, tokenizer)\n",
    "            self.val_acc_max_stance = val_acc_stance\n",
    "            self.val_acc_max_ideology = val_acc_ideology\n",
    "            self.counter = 0\n",
    "\n",
    "            #self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, val_acc_stance, val_acc_ideology, model_save_state, model_save_path, model, tokenizer):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            print(f'Validation acc stance : ({self.val_acc_max_stance:.6f} --> {val_acc_stance:.6f}).  Saving model ...')\n",
    "            print(f'Validation acc ideology : ({self.val_acc_max_ideology:.6f} --> {val_acc_ideology:.6f}).  Saving model ...')\n",
    "        #torch.save(model.module.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "        torch.save(model_save_state, model_save_path)\n",
    "        \n",
    "        \n",
    "        #model.save_pretrained('model_save/')\n",
    "        #tokenizer.save_pretrained('model_save/')\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        #torch.save(model, './model_save/entire_model.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02068398-ff74-4c14-b8df-6aff34a4a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStoppingUnigram:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, model_save_state, model_save_path, model, tokenizer):\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model_save_state, model_save_path, model, tokenizer)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model_save_state, model_save_path, model, tokenizer)\n",
    "            self.counter = 0\n",
    "\n",
    "            #self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model_save_state, model_save_path, model, tokenizer):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        #torch.save(model.module.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "        torch.save(model_save_state, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138bf3e-5447-4c75-801b-970c7a7663d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
