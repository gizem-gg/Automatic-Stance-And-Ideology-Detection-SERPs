{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49069301-54bc-42f4-bdac-650c73aca1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_seed():\n",
    "    return int.from_bytes(os.urandom(4), \"big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b1a74-32de-4797-8728-8442b0a4ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_more_notrel_docs(df):\n",
    "    #path = './dataset/batches_cleaned/stance/Latest_Merged_Batches.tsv'\n",
    "    \n",
    "    #df = pd.read_csv(path, delimiter='\\t', header = 0, names=['qID', 'docID', 'stance', 'docCont', 'Q', 'title'])\n",
    "    #df['docCont'] = df['docCont'].str.lower()\n",
    "    #df['Q'] = df['Q'].str.lower()\n",
    "    #df['title'] = df['title'].str.lower()\n",
    "    \n",
    "    df_pro = df[df['stance'] == \"Pro\"]\n",
    "    df_agst = df[df['stance'] == \"Agst\"]\n",
    "    df_neut = df[df['stance'] == \"Neut\"]\n",
    "    df_na = df[df['stance'] == \"Not-rel\"]\n",
    "    \n",
    "    pro_len = df_pro.shape[0]\n",
    "    agst_len = df_agst.shape[0]\n",
    "    neut_len = df_neut.shape[0]\n",
    "    not_rel_len = df_na.shape[0]\n",
    "    \n",
    "    print(\"Pro\", df_pro.shape[0])\n",
    "    print(\"Agst\", df_agst.shape[0])\n",
    "    print(\"Neut\", df_neut.shape[0])\n",
    "    print(\"Not-rel\", df_na.shape[0])\n",
    "    \n",
    "    print(\"before create more not-rel documents\")\n",
    "    \n",
    "    #neut_len = 800\n",
    "    rel_len = pro_len + agst_len + neut_len\n",
    "    all_len = rel_len + not_rel_len\n",
    "    \n",
    "    print(\"There are \" + str(rel_len) + \" rel instances in the original dataset!\")\n",
    "    print(\"There are \" + str(not_rel_len) + \" not-rel instances in the original dataset!\")\n",
    "    if rel_len > not_rel_len:\n",
    "        rest = rel_len - not_rel_len\n",
    "    else:\n",
    "        print(\"not_rel is bigger!\")\n",
    "        \n",
    "    print(\"There are \" + str(rest) + \" instances to be created!\")\n",
    "  \n",
    "     \n",
    "    path_queries = './dataset/batches_cleaned/stance/queries.tsv'\n",
    "    df_queries = pd.read_csv(path_queries, delimiter='\\t', header = 0, names=['qID', 'Q'])      \n",
    "    \n",
    "    \n",
    "    df_pro = df[df['stance'] == \"Pro\"]\n",
    "    df_agst = df[df['stance'] == \"Agst\"]\n",
    "    df_neut = df[df['stance'] == \"Neut\"]\n",
    "    df_na = df[df['stance'] == \"Not-rel\"]\n",
    "    \n",
    "    df_new = df_pro.append(df_agst, ignore_index = True)\n",
    "    df_new = df_new.append(df_neut, ignore_index = True)\n",
    "    #df_new = df_new.append(df_neut.sample(800), ignore_index = True)\n",
    "    df_new = df_new.append(df_na, ignore_index = True)\n",
    "\n",
    "    \n",
    "    tot_count = rest #old:900\n",
    "    for idx in range(0, tot_count):\n",
    "        valueRow = randint(1, all_len)\n",
    "        valueQ = randint(1, 57)\n",
    "        \n",
    "        old_doc_inst = df.iloc[valueRow-1]\n",
    "        \n",
    "        while old_doc_inst['stance'] == \"Not-rel\":\n",
    "            valueRow = randint(1, all_len)\n",
    "            valueQ = randint(1, 57)\n",
    "            \n",
    "            old_doc_inst = df.iloc[valueRow-1]\n",
    "        \n",
    "        new_inst = old_doc_inst\n",
    "        new_q_row = df_queries.iloc[valueQ-1]\n",
    "\n",
    "        new_qID = new_q_row['qID']\n",
    "        while new_qID == old_doc_inst['qID']:\n",
    "            valueQ = randint(1, 57)\n",
    "            new_q_row = df_queries.iloc[valueQ-1]\n",
    "            new_qID = new_q_row['qID']\n",
    "                \n",
    "        new_inst['qID'] = new_qID\n",
    "        new_inst['Q'] = new_q_row['Q']\n",
    "        new_inst['stance'] = \"Not-rel\"\n",
    "        new_inst['ideology'] = 'No'\n",
    "            \n",
    "        df_new = df_new.append(new_inst, ignore_index = True)\n",
    "        \n",
    "    print(df_new.shape[0])\n",
    "\n",
    "    df_pro = df_new[df_new['stance'] == \"Pro\"]\n",
    "    df_agst = df_new[df_new['stance'] == \"Agst\"]\n",
    "    df_neut = df_new[df_new['stance'] == \"Neut\"]\n",
    "    df_na = df_new[df_new['stance'] == \"Not-rel\"]\n",
    "    \n",
    "    print(\"Pro\", df_pro.shape[0])\n",
    "    print(\"Agst\", df_agst.shape[0])\n",
    "    print(\"Neut\", df_neut.shape[0])\n",
    "    print(\"Not-rel\", df_na.shape[0])\n",
    "    \n",
    "    print(\"after create more not-rel documents\")\n",
    "    \n",
    "    \n",
    "    df_new.to_csv('./dataset/batches_cleaned/stance/Final_Dataset_AddedNotRelated.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ed32d-d226-404b-ba13-afc4130e89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
    "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
    "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
    "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
    "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
    "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
    "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
    "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
    "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
    "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
    "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
    "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
    "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
    "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
    "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
    "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb0382-7936-4357-b3a8-3f373a5eb772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_train(df, dfVal, dfTest, lim_unigram):\n",
    "    \n",
    "    id_ref = {}\n",
    "    claims_t = []    \n",
    "    headlines_t = []\n",
    "    headlines_track_t = {}\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        claim = instance['Q']\n",
    "\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        claim = instance['Q']      \n",
    "        headline = instance['title'] + \" \" + instance['docCont']\n",
    "             \n",
    "        if headline not in headlines_track_t:\n",
    "            headlines_t.append(headline)\n",
    "            headlines_track_t[headline] = 1\n",
    "        if claim not in claims_t:\n",
    "            claims_t.append(claim)\n",
    "            \n",
    "    claims_v = []\n",
    "    headlines_v = []\n",
    "    headlines_track_v = {}\n",
    "    for index, instance in dfVal.iterrows():\n",
    "        claim = instance['Q']        \n",
    "        headline = instance['title'] + \" \" + instance['docCont']\n",
    "        \n",
    "        if headline not in headlines_track_v:\n",
    "            headlines_v.append(headline)\n",
    "            headlines_track_v[headline] = 1\n",
    "        if claim not in claims_v:\n",
    "            claims_v.append(claim)            \n",
    "\n",
    "      \n",
    "    ####################\n",
    "            \n",
    "    claims_test = []  \n",
    "    headlines_test = []\n",
    "    headlines_track_test = {}\n",
    "    for index, instance in dfTest.iterrows():\n",
    "        claim = instance['Q']\n",
    "        headline = instance['title'] + \" \" + instance['docCont']\n",
    "            \n",
    "        if headline not in headlines_track_test:\n",
    "            headlines_test.append(headline)\n",
    "            headlines_track_test[headline] = 1\n",
    "        \n",
    "        if claim not in claims_test:\n",
    "            claims_test.append(claim)\n",
    "\n",
    "            \n",
    "        # Create reference dictionary\n",
    "    for i, elem in enumerate(headlines_t + claims_t):\n",
    "        id_ref[elem] = i\n",
    "\n",
    "        \n",
    "    # Create vectorizers and BOW and TF arrays for train set\n",
    "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
    "    bow = bow_vectorizer.fit_transform(claims_t + headlines_t)  # Train set only\n",
    "    \n",
    "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
    "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
    "        fit(headlines_t + claims_t + headlines_v + claims_v + headlines_test + claims_test)  # Train and test sets\n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    \n",
    "    headline_tfidf_track = {}\n",
    "    claim_tfidf_track = {}\n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    train_set = []\n",
    "    print(len(df))\n",
    "    \n",
    "    for index, instance in df.iterrows():\n",
    "        headline = instance['title'] + \" \" + instance['docCont']\n",
    "        claim = instance['Q']\n",
    "        \n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        headline_tf = tfreq[id_ref[headline]].reshape(1, -1)\n",
    "        claim_tf = tfreq[id_ref[claim]].reshape(1, -1)\n",
    "        \n",
    "        if headline not in headline_tfidf_track:\n",
    "            head_tfidf = tfidf_vectorizer.transform([headline]).toarray()\n",
    "            headline_tfidf_track[headline] = head_tfidf\n",
    "        else:\n",
    "            head_tfidf = headline_tfidf_track[headline]\n",
    "            \n",
    "        if claim not in claim_tfidf_track:\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray()\n",
    "            claim_tfidf_track[claim] = claim_tfidf\n",
    "        else:\n",
    "            claim_tfidf = claim_tfidf_track[claim]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(head_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "\n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        train_set.append(feat_vec)\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features_serp_onlytrain/train_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features_serp_onlytrain/train_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features_serp_onlytrain/train_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features_serp_onlytrain/train_hand.npy')\n",
    "    \n",
    "    train_features = np.squeeze(np.c_[X_refuting, X_polarity,X_overlap,X_hand])#\n",
    "    train_set = np.squeeze(np.c_[train_set,train_features])\n",
    "    ####### preprocessing\n",
    "    train_set = np.asarray(train_set)\n",
    "    train_mean = np.mean(train_set, axis = 0)\n",
    "    train_set = train_set-train_mean\n",
    "    \n",
    "    return train_set, train_mean, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20afbe5a-dddb-4b0c-9f8e-abeacd9526a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_val(dfVal, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m):\n",
    "    \n",
    "    headlines_track = {}\n",
    "    claims_track = {}\n",
    "    \n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    test_set = []\n",
    "    \n",
    "    for index, instance in dfVal.iterrows():\n",
    "        headline = instance['title'] + \" \" + instance['docCont']\n",
    "        claim = instance['Q']\n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        if headline not in headlines_track:\n",
    "            head_bow = bow_vectorizer.transform([headline]).toarray()\n",
    "            headline_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
    "            headline_tfidf = tfidf_vectorizer.transform([headline]).toarray().reshape(1, -1)\n",
    "            headlines_track[headline] = (headline_tf, headline_tfidf)\n",
    "        else:\n",
    "            headline_tf = headlines_track[headline][0]\n",
    "            headline_tfidf = headlines_track[headline][1]\n",
    "            \n",
    "        if claim not in claims_track:\n",
    "            claim_bow = bow_vectorizer.transform([claim]).toarray()\n",
    "            claim_tf = tfreq_vectorizer.transform(claim_bow).toarray()[0].reshape(1, -1)\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray().reshape(1, -1)\n",
    "            claims_track[headline] = (claim_tf, claim_tfidf)\n",
    "        else:\n",
    "            claim_tf = claims_track[claim][0]\n",
    "            claim_tfidf = claims_track[claim][1]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(headline_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "            \n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        test_set.append(feat_vec)\n",
    "        \n",
    "        \n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features_serp_onlytrain/val_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features_serp_onlytrain/val_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features_serp_onlytrain/val_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features_serp_onlytrain/val_hand.npy')\n",
    "    \n",
    "    test_features = np.squeeze(np.c_[X_refuting,X_polarity,X_overlap,X_hand])#\n",
    "    test_set = np.squeeze(np.c_[test_set, test_features])\n",
    "    ####### preprocessing\n",
    "    test_set = np.asarray(test_set)\n",
    "    test_set = (test_set-m)#/(std+0.0001)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97de2b8-41f3-4162-aaf1-ec19a19cf037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_test(dfTest, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m):\n",
    "    \n",
    "    headlines_track = {}\n",
    "    claims_track = {}\n",
    "    \n",
    "    headline_all = []\n",
    "    claim_all = []\n",
    "    cos_track = {}\n",
    "    \n",
    "    test_set = []\n",
    "    \n",
    "    for index, instance in dfTest.iterrows():\n",
    "        headline = instance['title'] + \" \" + instance['docCont']\n",
    "        claim = instance['Q']\n",
    "        headline_all.append(headline)\n",
    "        claim_all.append(claim)\n",
    "        \n",
    "        if headline not in headlines_track:\n",
    "            head_bow = bow_vectorizer.transform([headline]).toarray()\n",
    "            headline_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
    "            headline_tfidf = tfidf_vectorizer.transform([headline]).toarray().reshape(1, -1)\n",
    "            headlines_track[headline] = (headline_tf, headline_tfidf)\n",
    "        else:\n",
    "            headline_tf = headlines_track[headline][0]\n",
    "            headline_tfidf = headlines_track[headline][1]\n",
    "            \n",
    "        if claim not in claims_track:\n",
    "            claim_bow = bow_vectorizer.transform([claim]).toarray()\n",
    "            claim_tf = tfreq_vectorizer.transform(claim_bow).toarray()[0].reshape(1, -1)\n",
    "            claim_tfidf = tfidf_vectorizer.transform([claim]).toarray().reshape(1, -1)\n",
    "            claims_track[headline] = (claim_tf, claim_tfidf)\n",
    "        else:\n",
    "            claim_tf = claims_track[claim][0]\n",
    "            claim_tfidf = claims_track[claim][1]\n",
    "            \n",
    "        if (headline, claim) not in cos_track:\n",
    "            tfidf_cos = cosine_similarity(headline_tfidf, claim_tfidf)[0].reshape(1, 1)\n",
    "            cos_track[(headline, claim)] = tfidf_cos\n",
    "        else:\n",
    "            tfidf_cos = cos_track[(headline, claim)]\n",
    "            \n",
    "        feat_vec = np.squeeze(np.c_[headline_tf, claim_tf, tfidf_cos])\n",
    "        test_set.append(feat_vec)\n",
    "        \n",
    "        \n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, headline_all, claim_all, 'features_serp_onlytrain/test_overlap.npy')\n",
    "    X_refuting = gen_or_load_feats(refuting_features, headline_all, claim_all, 'features_serp_onlytrain/test_refuting.npy')\n",
    "    X_polarity = gen_or_load_feats(polarity_features, headline_all, claim_all, 'features_serp_onlytrain/test_polarity.npy')\n",
    "    X_hand = gen_or_load_feats(hand_features, headline_all, claim_all, 'features_serp_onlytrain/test_hand.npy')\n",
    "    \n",
    "    test_features = np.squeeze(np.c_[X_refuting,X_polarity,X_overlap,X_hand])#\n",
    "    test_set = np.squeeze(np.c_[test_set, test_features])\n",
    "    ####### preprocessing\n",
    "    test_set = np.asarray(test_set)\n",
    "    test_set = (test_set-m)#/(std+0.0001)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b36b66-8da9-47d2-aada-75c56a34e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_utils():\n",
    "    # Get the GPU device name.\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    # The device name should look like the following:\n",
    "    if device_name == '/device:GPU:0':\n",
    "        print('Found GPU at: {}'.format(device_name))\n",
    "    else:\n",
    "        raise SystemError('GPU device not found')\n",
    "\n",
    "    device = None\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07012518-83ad-4bd7-83d6-7de1a5b1fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e78690c-c5fe-4eb9-b617-6021bd1236ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_stance(path):\n",
    "    df = pd.read_csv(path, delimiter='\\t', header = 0, names=['qID', 'docID', 'stance', 'ideology', 'docCont', 'Q', 'title'])    \n",
    "    \n",
    "    #df[\"stance\"] = df[\"stance\"].replace({\"Pro\": \"0\", \"Agst\": \"1\", \"Neut\": \"2\", \"Not-rel\": \"3\"})\n",
    "    #df[\"stance\"] = df[\"stance\"].replace({\"Not-rel\": \"Notrel\"})\n",
    "\n",
    "    df['docCont'] = df['docCont'].str.lower()\n",
    "    df['Q'] = df['Q'].str.lower()\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    \n",
    "    df = df.astype({'docCont': 'str'})\n",
    "    df = df.astype({'Q': 'str'})\n",
    "    df = df.astype({'title': 'str'})\n",
    "    \n",
    "    print(df['ideology'])\n",
    "    print(\"**********\")\n",
    "    \n",
    "    #df = df.drop('ideology', axis=1).copy(deep=True)\n",
    "    \n",
    "    print(df.iloc[0])\n",
    "    \n",
    "    print(df.dtypes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e3cba-65bc-4899-87fb-a100ff244be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def predict_classwise_stance_ideology(P_related, P_unrelated, P_relatedness, P_stance, stance_labels):\n",
    "\n",
    "    #P_related = torch.reshape(P_relatedness[:, 0], (-1, 1))\n",
    "    #P_unrelated = torch.reshape(P_relatedness[:, 1], (-1, 1))\n",
    "    \n",
    "    tmp1 = P_stance[:,:3]\n",
    "    tmp2 = torch.reshape(torch.sum(tmp1,dim=1),[-1,1])\n",
    "    tmp3 = torch.cat([tmp2,tmp2,tmp2],dim=1)\n",
    "    tmp4 = torch.cat([P_related, P_related, P_related],dim=1)\n",
    "    tmp5 = torch.div(tmp1,tmp3)\n",
    "    tmp6 = tmp5*tmp4\n",
    "    prob = torch.cat([tmp6,P_unrelated],1)#tmp6\n",
    "    \n",
    "    target_labels = torch.argmax(torch.abs(stance_labels), 1)\n",
    "    predict_labels = torch.argmax(prob, 1)\n",
    "    \n",
    "    P_oneside = torch.reshape(P_stance[:, :2], (-1, 2))\n",
    "    P_noside = torch.reshape(P_stance[:, 2], (-1, 1))\n",
    "    \n",
    "    \n",
    "    agree_true = 0\n",
    "    agree_total = 0\n",
    "    \n",
    "    disagree_true = 0\n",
    "    disagree_total = 0\n",
    "    \n",
    "    discuss_true = 0\n",
    "    discuss_total = 0\n",
    "    \n",
    "    unrelated_true = 0\n",
    "    unrelated_total = 0\n",
    "    \n",
    "    for idx, true_label in enumerate(target_labels):\n",
    "        predict_label = predict_labels[idx] \n",
    "        if true_label == 0:\n",
    "            agree_total += 1\n",
    "            if predict_label == 0:\n",
    "                agree_true += 1\n",
    "        elif true_label == 1:\n",
    "            disagree_total += 1\n",
    "            if predict_label == 1:\n",
    "                disagree_true += 1\n",
    "        elif true_label == 2:\n",
    "            discuss_total += 1\n",
    "            if predict_label == 2:\n",
    "                discuss_true += 1\n",
    "        elif true_label == 3:\n",
    "            unrelated_total += 1\n",
    "            if predict_label == 3:\n",
    "                unrelated_true += 1\n",
    "    accuracy_agree = 0\n",
    "    if agree_true != 0:\n",
    "        accuracy_agree = agree_true/agree_total\n",
    "        \n",
    "    accuracy_disagree = 0\n",
    "    if disagree_true != 0:\n",
    "        accuracy_disagree = disagree_true/disagree_total\n",
    "    \n",
    "    accuracy_discuss = 0\n",
    "    if discuss_true != 0:\n",
    "        accuracy_discuss = discuss_true/discuss_total\n",
    "        \n",
    "    accuracy_notrelated = 0\n",
    "    if unrelated_true != 0:\n",
    "        accuracy_notrelated = unrelated_true/unrelated_total\n",
    "    \n",
    "    true_total = agree_true + disagree_true + discuss_true + unrelated_true\n",
    "    true_predict_count = len((torch.eq(predict_labels, target_labels)).nonzero().flatten())\n",
    "    accuracy = true_predict_count / len(predict_labels)\n",
    "    #print(\"*************\")\n",
    "    #print(\"Total True\")\n",
    "    #print(true_total)\n",
    "    #print(agree_true, agree_total)\n",
    "    #print(disagree_true, disagree_total)\n",
    "    #print(discuss_true, discuss_total)\n",
    "    #print(unrelated_true, unrelated_total)\n",
    "    \n",
    "    return accuracy, agree_true, disagree_true, discuss_true, unrelated_true, true_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1566e-60ab-49b8-aa4c-e15184e8520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_class_num(df):\n",
    "    df_pro = df[df['stance'] == \"Pro\"]\n",
    "    df_agst = df[df['stance'] == \"Agst\"]\n",
    "    df_neut = df[df['stance'] == \"Neut\"]\n",
    "    df_na = df[df['stance'] == \"Not-rel\"]\n",
    " \n",
    "    return [df_pro.shape[0], df_agst.shape[0], df_neut.shape[0], df_na.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f20629-4738-4eac-a5fe-7a3b9681f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stance_ideology(s_labels):\n",
    "    t_relatedness = []\n",
    "    t_stance = []\n",
    "    \n",
    "    t_existedstance = []\n",
    "    t_ideology = []\n",
    "    \n",
    "    t_mmd_symbol = []\n",
    "    t_mmd_symbol_ = []\n",
    "    \n",
    "    for idx, s_label in enumerate(s_labels):\n",
    "        if s_label == \"Not-rel\": #unrelated\n",
    "            t_relatedness.append([0,1])\n",
    "            t_stance.append([0,0,0,-1])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(1)\n",
    "            t_existedstance.append([0,1])\n",
    "            t_ideology.append([0,0,-1])\n",
    "        elif s_label == \"Pro\": #agree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([1,0,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "            t_existedstance.append([1,0])\n",
    "        elif s_label == \"Agst\": #disagree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,1,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "            t_existedstance.append([1,0])\n",
    "        elif s_label == \"Neut\": #discuss\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,0,1,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "            t_existedstance.append([0,1])\n",
    "            t_ideology.append([0,0,1])\n",
    "        else:\n",
    "            print(\"Error-labels\", s_label, i_label)\n",
    "            \n",
    "    \n",
    "    t_relatedness = torch.as_tensor(t_relatedness, dtype=torch.int32)\n",
    "    t_stance = torch.as_tensor(t_stance, dtype=torch.int32)\n",
    "    t_existedstance = torch.as_tensor(t_existedstance, dtype=torch.int32)\n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    t_mmd_symbol  = torch.as_tensor(t_mmd_symbol, dtype=torch.float32)\n",
    "    t_mmd_symbol_ = torch.as_tensor(t_mmd_symbol_, dtype=torch.float32)\n",
    "    \n",
    "    return t_relatedness, t_stance, t_mmd_symbol, t_mmd_symbol_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99123487-8c44-4294-9362-cd2ba01aa462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stance_ideology_test(s_labels):\n",
    "    t_relatedness = []\n",
    "    t_stance = []\n",
    "    \n",
    "    t_existedstance = []\n",
    "    t_ideology = []\n",
    "    \n",
    "    t_mmd_symbol = []\n",
    "    t_mmd_symbol_ = []\n",
    "\n",
    "    for idx, s_label in enumerate(s_labels):\n",
    "        i_label = \"Con\"\n",
    "        if s_label == \"Not-rel\": #unrelated\n",
    "            t_relatedness.append([0,1])\n",
    "            t_stance.append([0,0,0,-1])\n",
    "            t_mmd_symbol.append(0)\n",
    "            t_mmd_symbol_.append(1)\n",
    "            t_existedstance.append([0,1])\n",
    "            t_ideology.append([0,0,-1])\n",
    "        elif s_label == \"Pro\": #agree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([1,0,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "            t_existedstance.append([1,0])\n",
    "            if i_label == \"Con\":\n",
    "                t_ideology.append([1,0,0])\n",
    "            elif i_label == \"Lib\":\n",
    "                t_ideology.append([0,1,0])\n",
    "            else:\n",
    "                t_ideology.append([0,0,-1])\n",
    "        elif s_label == \"Agst\": #disagree\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,1,0,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "            t_existedstance.append([1,0])\n",
    "            if i_label == \"Con\":\n",
    "                t_ideology.append([1,0,0])\n",
    "            elif i_label == \"Lib\":\n",
    "                t_ideology.append([0,1,0])\n",
    "            else:\n",
    "                t_ideology.append([0,0,-1])\n",
    "        elif s_label == \"Neut\": #discuss\n",
    "            t_relatedness.append([1,0])\n",
    "            t_stance.append([0,0,1,0])\n",
    "            t_mmd_symbol.append(1)\n",
    "            t_mmd_symbol_.append(0)\n",
    "            t_existedstance.append([0,1])\n",
    "            t_ideology.append([0,0,1])\n",
    "        else:\n",
    "            print(\"Error-labels\", s_label, i_label)\n",
    "            \n",
    "    \n",
    "    t_relatedness = torch.as_tensor(t_relatedness, dtype=torch.int32)\n",
    "    t_stance = torch.as_tensor(t_stance, dtype=torch.int32)\n",
    "    t_existedstance = torch.as_tensor(t_existedstance, dtype=torch.int32)\n",
    "    t_ideology = torch.as_tensor(t_ideology, dtype=torch.int32)\n",
    "    \n",
    "    t_mmd_symbol  = torch.as_tensor(t_mmd_symbol, dtype=torch.float32)\n",
    "    t_mmd_symbol_ = torch.as_tensor(t_mmd_symbol_, dtype=torch.float32)\n",
    "    \n",
    "    return t_relatedness, t_stance, t_mmd_symbol, t_mmd_symbol_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3f7e4-a712-47f9-a7cc-7027f11f97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stance_ideology_meta(s_labels):\n",
    "    \n",
    "    t_stance = []\n",
    "    for idx, s_label in enumerate(s_labels):\n",
    "        if s_label == \"Not-rel\": #unrelated\n",
    "            t_stance.append([0,0,0,-1])\n",
    "        elif s_label == \"Pro\": #agree\n",
    "            t_stance.append([1,0,0,0])\n",
    "        elif s_label == \"Agst\": #disagree\n",
    "            t_stance.append([0,1,0,0])\n",
    "        elif s_label == \"Neut\": #discuss\n",
    "            t_stance.append([0,0,1,0])\n",
    "        else:\n",
    "            print(\"Error-labels\", s_label)\n",
    "\n",
    "    return t_stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205146f5-f40d-4f3c-9978-b6de28d99ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concanListStringsLonger(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" GIZEM \" + list2[idx])\n",
    "    return list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825f55f-adf7-467a-ac25-3951d2a5a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concanListStrings(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" \" + list2[idx])\n",
    "    return list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6a38b-de40-4c7a-9bad-bb29d375eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concanListStrings_sep(list1, list2):\n",
    "    list3 = []\n",
    "    myLen1 = len(list1)\n",
    "    if myLen1 != len(list2):\n",
    "        print(\"Length - error\")\n",
    "    for idx in range(0, myLen1):\n",
    "        list3.append(list1[idx] + \" [SEP] \" + list2[idx])\n",
    "\n",
    "    return list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcaa9b6-72e0-43d9-b3cf-8af32f6d1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the datasets with the different fields.\n",
    "def generate_datasets(df):\n",
    "\n",
    "    sentencesQuery = df.Q.values #claim in this case\n",
    "    sentencesTitle = df.title.values\n",
    "    labels = df.stance.values\n",
    "    labels_ideology = df.ideology.values\n",
    "    #df.ideology.values\n",
    "    \n",
    "    sentencesCont = df.docCont.values\n",
    "\n",
    "    sentencesQueryTitle = concanListStrings(sentencesQuery, sentencesTitle)\n",
    "    sentencesQueryTitleCont = concanListStringsLonger(sentencesQueryTitle, sentencesCont)\n",
    "\n",
    "    return sentencesQueryTitle, sentencesQueryTitleCont, labels, labels_ideology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e4231-98b3-45d5-8c2d-822102783d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, RobertaModel, DistilBertModel\n",
    "class StanceDetectionUnigramClass(torch.nn.Module):\n",
    "    def __init__(self, datasetUsed):\n",
    "        super(StanceDetectionUnigramClass, self).__init__()\n",
    "        input_size = len(datasetUsed[0])\n",
    "        hidden_size_initial = 200\n",
    "        hidden_size = 200\n",
    "        mmd_size = 50\n",
    "        dropout_prob = 0.6\n",
    "        dropout_prob2 = 0.1\n",
    "        relatedness_size = 2\n",
    "        classes_size = 4\n",
    "        ideology_class_size = 3\n",
    "        #agreement_size = 3\n",
    "        #self.input_pl = BertForPreTraining.from_pretrained(modelUsed) #input\n",
    "        #self.input_pl = BertModel.from_pretrained(modelUsed)\n",
    "        #self.input_pl = RobertaModel.from_pretrained(modelUsed)\n",
    "        #self.input_pl = DistilBertModel.from_pretrained(modelUsed)\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_size_initial)\n",
    "        self.l2 = torch.nn.Linear(hidden_size_initial, hidden_size)\n",
    "        self.l3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn1_hidden = torch.nn.BatchNorm1d(hidden_size_initial, momentum=0.05)\n",
    "        self.bn2_hidden = torch.nn.BatchNorm1d(hidden_size, momentum=0.05)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout_prob2)\n",
    "\n",
    "        self.theta_d = torch.nn.Linear(hidden_size, mmd_size)\n",
    "        self.bn1_theta = torch.nn.BatchNorm1d(mmd_size, momentum=0.05)\n",
    "        \n",
    "        self.probability = torch.nn.Linear(hidden_size, relatedness_size)\n",
    "        self.output_prob = torch.nn.Softmax(dim = 1)\n",
    "        \n",
    "        self.stance = torch.nn.Linear(hidden_size + relatedness_size - 1, classes_size)\n",
    "        self.ideology = torch.nn.Linear(hidden_size + classes_size - 2, ideology_class_size)\n",
    "        \n",
    "        #for param in self.input_pl.embeddings.parameters():\n",
    "            #param.requires_grad = False\n",
    "        \n",
    "        #for param in self.input_pl[2][0:5].parameters():\n",
    "            #param.requires_grad = False\n",
    "\n",
    "        #self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, mmd_pl, mmd_pl_):\n",
    "        relatedness_size = 2\n",
    "        classes_size = 4\n",
    "        ideology_class_size = 3\n",
    "        \n",
    "        #hidden layer\n",
    "        hidden_state = self.l1(input_ids)\n",
    "        hidden_state_normalized = self.bn1_hidden(hidden_state)\n",
    "        hidden_state_normalized = torch.nn.ReLU()(hidden_state_normalized)\n",
    "        hidden_layer= self.dropout2(hidden_state_normalized)\n",
    "        \n",
    "                #hidden layer\n",
    "        hidden_state = self.l2(hidden_layer)\n",
    "        hidden_state_normalized = self.bn1_hidden(hidden_state)\n",
    "        hidden_state_normalized = torch.nn.ReLU()(hidden_state_normalized)\n",
    "        hidden_layer= self.dropout2(hidden_state_normalized)\n",
    "    \n",
    "        #mmd layer        \n",
    "        theta_d = self.theta_d(hidden_layer)\n",
    "        theta_d_normalized = self.bn1_theta(theta_d)\n",
    "        theta_d_normalized = torch.nn.ReLU()(theta_d_normalized)\n",
    "        theta_d_layer= self.dropout2(theta_d_normalized)\n",
    "        \n",
    "        \n",
    "        n1 = torch.sum(mmd_pl, dim = 0) + 1e-10\n",
    "        n2 = torch.sum(mmd_pl_, dim = 0)  + 1e-10\n",
    "        aa = torch.reshape(mmd_pl, (-1,1))\n",
    "        bb = torch.reshape(mmd_pl_, (-1,1))\n",
    "        \n",
    "        #calculate mmd_loss                  \n",
    "        d1 = torch.div(torch.sum(torch.mul(theta_d_layer, aa), dim=1), n1)\n",
    "        d2 = torch.div(torch.sum(torch.mul(theta_d_layer, bb), dim=1), n2)\n",
    "                             \n",
    "        mmd_loss = torch.sum(d1 - d2)\n",
    "\n",
    "        #probability layer\n",
    "        relatedness_state = self.probability(hidden_layer)\n",
    "        relatedness_flat = self.dropout2(relatedness_state)\n",
    "        \n",
    "        relatedness_flat_reshaped = torch.reshape(relatedness_flat, (-1, relatedness_size))\n",
    "        P_relatedness = self.output_prob(relatedness_flat_reshaped)\n",
    "        #P_relatedness = relatedness_flat_reshaped\n",
    "        \n",
    "        P_related = torch.reshape(P_relatedness[:, 0], (-1, 1))\n",
    "        P_unrelated = torch.reshape(P_relatedness[:, 1], (-1, 1))\n",
    "        \n",
    "        #stance layer\n",
    "        concat_fea = torch.cat([hidden_layer, P_related], dim = 1)\n",
    "        stance_state = self.stance(concat_fea) #batch size x classes_size\n",
    "        stance_flat = self.dropout2(stance_state) #batch size x classes_size\n",
    "        \n",
    "        stance_flat_reshaped = torch.reshape(stance_flat, (-1, classes_size))\n",
    "        P_stance = self.output_prob(stance_flat_reshaped) \n",
    "        \n",
    "\n",
    "        return mmd_loss, P_relatedness, P_stance, P_related, P_unrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059c4b8-ad3f-465d-98f1-fb7fe6a31119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "def prepare_for_training_stance_ideology(instancesTrain, labelsTrain, instancesVal, labelsVal, modelUsed, batch_size=16, epochs = 50, num_warmup_steps=0, learning_rate=5e-5):\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "\n",
    "    from transformers import BertForSequenceClassification, AdamW, BertConfig, RobertaConfig, AutoModelWithLMHead\n",
    "    from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification\n",
    "    \n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    \n",
    "    t_train_relatedness, t_train_stance, t_train_mmd_symbol, t_train_mmd_symbol_ = preprocess_stance_ideology(labelsTrain)\n",
    "    t_instancesTrain  = torch.as_tensor(instancesTrain, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    print(t_instancesTrain.shape, t_train_stance.shape)\n",
    "    datasetTrain = TensorDataset(t_instancesTrain, t_train_relatedness, t_train_stance, t_train_mmd_symbol, t_train_mmd_symbol_)\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    t_val_relatedness, t_val_stance, t_val_mmd_symbol, t_val_mmd_symbol_ = preprocess_stance_ideology_test(labelsVal)\n",
    "    t_instancesVal = torch.as_tensor(instancesVal, dtype=torch.float32)\n",
    "    datasetVal = TensorDataset(t_instancesVal, t_val_relatedness, t_val_stance, t_val_mmd_symbol, t_val_mmd_symbol_)\n",
    "    \n",
    "    \n",
    "    model = StanceDetectionUnigramClass(instancesTrain)\n",
    "    \n",
    "        \n",
    "    for name, module in model.named_modules():\n",
    "        if name in ['drop1', 'dropout2', 'dropout'] and isinstance(module, torch.nn.Dropout):\n",
    "            setattr(model, name, torch.nn.Dropout(0))\n",
    "        if name in ['l1', 'l2'] and isinstance(module, torch.nn.Linear):\n",
    "            target_state_dict = module.state_dict()\n",
    "            bias = True if module.bias is not None else False\n",
    "            new_module = MixLinear(module.in_features, module.out_features, \n",
    "                                   bias, target_state_dict['weight'], 0.5)\n",
    "            new_module.load_state_dict(target_state_dict)\n",
    "            setattr(model, name, new_module)\n",
    "    print(\"After applying mixout\")\n",
    "    print(model)\n",
    "    \n",
    "\n",
    "    #print(model)\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    #model.cuda()\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=1e-4,\n",
    "                  correct_bias=True\n",
    "               )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8\n",
    "        )\n",
    "    batch_size = batch_size\n",
    "\n",
    "\n",
    "    from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "    # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "    # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "    # training data.\n",
    "    epochs = epochs\n",
    "\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    schedulerOld = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    \n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps, num_cycles = 5)\n",
    "    \n",
    "    return model, datasetTrain, datasetVal, optimizer, schedulerOld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacb23a-fdf1-496b-98be-605797c09441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_to(optim, device):\n",
    "    for param in optim.state.values():\n",
    "        # Not sure there are any global tensors in the state dict\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            param.data = param.data.to(device)\n",
    "            if param._grad is not None:\n",
    "                param._grad.data = param._grad.data.to(device)\n",
    "        elif isinstance(param, dict):\n",
    "            for subparam in param.values():\n",
    "                if isinstance(subparam, torch.Tensor):\n",
    "                    subparam.data = subparam.data.to(device)\n",
    "                    if subparam._grad is not None:\n",
    "                        subparam._grad.data = subparam._grad.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2140d3-ec17-4db0-aa03-befadd57ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_batches_datasets(datasetTrain, datasetVal, batch_size = 16):\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "        \n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(\n",
    "            datasetTrain,  # The training samples.\n",
    "            sampler =  RandomSampler(datasetTrain), # Select batches randomly\n",
    "            batch_size = batch_size, # Trains with this batch size., \n",
    "            num_workers=8, drop_last=True\n",
    "        )\n",
    "\n",
    "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(\n",
    "            datasetVal, # The validation samples.\n",
    "            sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "            batch_size = batch_size, # Evaluate with this batch size.\n",
    "            num_workers=8, drop_last=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "    #validation_dataloader = DataLoader(\n",
    "    #        datasetVal, # The validation samples.\n",
    "    #        sampler = SequentialSampler(datasetVal), # Pull out batches sequentially.\n",
    "    #        batch_size = batch_size, # Evaluate with this batch size.\n",
    "    #        num_workers=0, drop_last=True\n",
    "    #)\n",
    "    \n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75e1a3-b407-4334-8efb-3152a3d484b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "#import EarlyStopping\n",
    "def train_stance_ideology(train_nums, val_nums, model_save_path, model, tokenizer, datasetTrain, datasetVal, epochs, batch_size, optimizer, scheduler, patience, verbose, delta, seedVal, continue_train = False):\n",
    "    \n",
    "    pro_val_num = val_nums[0]\n",
    "    agst_val_num = val_nums[1]\n",
    "    neut_val_num = val_nums[2]\n",
    "    notrel_val_num = val_nums[3]\n",
    "    \n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    min_val_loss = 100\n",
    "    \n",
    "    relatedness_size = 2\n",
    "    classes_size = 4\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    loss_fct_stance = torch.nn.CrossEntropyLoss()\n",
    "    #loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    alpha = 1.3\n",
    "    beta = 1e-3\n",
    "    theta = 0\n",
    "    gamma = 0\n",
    "    \n",
    "    batch_size_max_once = 16\n",
    "\n",
    "    if batch_size < batch_size_max_once:\n",
    "        batch_size_max_once = batch_size\n",
    "        \n",
    "    accumulation_steps = batch_size/batch_size_max_once\n",
    "    \n",
    "    es = EarlyStopping(patience,verbose, delta)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    train_dataloader, validation_dataloader = return_batches_datasets(datasetTrain, datasetVal, batch_size_max_once)\n",
    "    \n",
    "    epoch_start = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        #multi-gpu\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    print(device)\n",
    "    \n",
    "    \n",
    "            \n",
    "    if continue_train:\n",
    "        checkpoint = torch.load('./model_save/fnc/model_fncbert.t7')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    \n",
    "     #pos_weight=torch.FloatTensor ([28.36 / 0.5090]\n",
    "    \n",
    "     #pos_weight = torch.tensor([1.0, 1.0, 1.0])\n",
    "     #pos_weight = pos_weight.to(device)\n",
    "     #criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    weights_ideology = torch.tensor([10, 5, 1.5]).to(device)   \n",
    "    weights_stance = torch.tensor([0.82, 0.85, 0.67, 0.3]).to(device) \n",
    "    loss_fct_relatedness_weighted = torch.nn.BCEWithLogitsLoss(pos_weight = weights_stance)\n",
    "    \n",
    "    # For each epoch...\n",
    "    batch_epoch_count = 1\n",
    "    for epoch_i in range(epoch_start, epoch_start + epochs):\n",
    "        \n",
    "        print(\"---------Epoch----------\" + str(epoch_i))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        #print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # For each batch of training data...\n",
    "        mini_batch_avg_loss = 0\n",
    "        #train_size = len(train_dataloader)\n",
    "        \n",
    "        if batch_epoch_count % 500 == 0:\n",
    "            batch_size = batch_size*2\n",
    "            accumulation_steps = int(batch_size/batch_size_max_once)\n",
    "        batch_epoch_count = batch_epoch_count + 1\n",
    "\n",
    "        #train_size = len(train_dataloader) / float(accumulation_steps)\n",
    "        \n",
    "        print(\"Batch Size: \" + str(batch_size))\n",
    "        print(float(accumulation_steps))\n",
    "        \n",
    "        #print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_relatedness = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                b_mmd_symbol = batch[3].to(device)\n",
    "                b_mmd_symbol_ = batch[4].to(device)\n",
    "        \n",
    "            \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            \n",
    "\n",
    "                mmd_loss, P_relatedness, P_stance, P_related, P_unrelated = model(input_ids = b_input_ids, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "\n",
    "            \n",
    "                relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "            \n",
    "                loss = relatedness_loss + alpha * stance_loss + beta * mmd_loss\n",
    "                loss = loss / accumulation_steps \n",
    "                total_train_loss += loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "                    \n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This is to help prevent the \"exploding gradients\" problem.\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                \n",
    "                # Update parameters and take a step using the computed gradient.\n",
    "                # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "                # modified based on their gradients, the learning rate, etc.\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Update the learning rate.\n",
    "                    scheduler.step()\n",
    "                \n",
    "                #for param_group in optimizer.param_groups:\n",
    "                #print(\"Learning Rate: \", optimizer.param_groups[\"lr\"])\n",
    "                \n",
    "                                \n",
    "                # Always clear any previously calculated gradients before performing a\n",
    "                # backward pass. PyTorch doesn't do this automatically because \n",
    "                # accumulating the gradients is \"convenient while training RNNs\". \n",
    "                # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "                    model.zero_grad()\n",
    "                    optimizer.zero_grad()       \n",
    "    \n",
    "        print(\"Learning rate: \", scheduler.get_last_lr())\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) * accumulation_steps\n",
    "    \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        #print(\"\")\n",
    "        print(\"  Average training loss: {0:.6f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        #print(\"\")\n",
    "        #print(\"Running Validation...\")\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy_stance = 0\n",
    "        total_eval_accuracy_ideology = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        \n",
    "        agree_val_true = 0\n",
    "        disagree_val_true = 0 \n",
    "        discuss_val_true = 0 \n",
    "        unrelated_val_true = 0\n",
    "        \n",
    "        con_test_acc = 0\n",
    "        lib_test_acc = 0\n",
    "        na_test_acc = 0\n",
    "        \n",
    "        total_true = 0\n",
    "        \n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_relatedness = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_mmd_symbol = batch[3].to(device)\n",
    "            b_mmd_symbol_ = batch[4].to(device)\n",
    "        \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "                mmd_loss, P_relatedness, P_stance, P_related, P_unrelated = model(input_ids = b_input_ids, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "                \n",
    "                #CrossEntropy Loss\n",
    "                relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "                stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "                \n",
    "                loss_val = relatedness_loss + alpha * stance_loss + beta * mmd_loss\n",
    "                total_eval_loss += loss_val.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                P_relatedness = P_relatedness.to('cpu')\n",
    "                b_relatedness = b_relatedness.to('cpu')\n",
    "                P_stance = P_stance.to('cpu')\n",
    "                b_labels = b_labels.to('cpu')\n",
    "                b_input_ids = b_input_ids.to('cpu')\n",
    "                P_related = P_related.to('cpu')\n",
    "                P_unrelated = P_unrelated.to('cpu')\n",
    "                \n",
    "\n",
    "                # Calculate the accuracy for this batch of test sentences, and\n",
    "                # accumulate it over all batches.\n",
    "                #total_eval_accuracy += predict(P_relatedness, P_stance, b_labels)\n",
    "                \n",
    "                \n",
    "                acc_list = predict_classwise_stance_ideology(P_related, P_unrelated, P_relatedness, P_stance, b_labels)\n",
    "                total_eval_accuracy_stance += acc_list[0]\n",
    "                ###\n",
    "                agree_val_true += acc_list[1]\n",
    "                disagree_val_true += acc_list[2]\n",
    "                discuss_val_true += acc_list[3]\n",
    "                unrelated_val_true += acc_list[4]\n",
    "                total_true += acc_list[5]\n",
    "        \n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy_stance = total_eval_accuracy_stance / len(validation_dataloader)\n",
    "\n",
    "        print(\"Avg Val Accuracy Stance: {0:.6f}\".format(avg_val_accuracy_stance))\n",
    "        print(\"Total True\")\n",
    "        print(total_true)\n",
    "        print(\"*************\")\n",
    "        avg_val_agree_accuracy = agree_val_true / pro_val_num\n",
    "        print(\"Avg Val Agree Accuracy: {0:.6f}\".format(avg_val_agree_accuracy))\n",
    "        avg_val_disagree_accuracy = disagree_val_true / agst_val_num\n",
    "        print(\"Avg Val Disagree Accuracy: {0:.6f}\".format(avg_val_disagree_accuracy))\n",
    "        avg_val_discuss_accuracy = discuss_val_true / neut_val_num\n",
    "        print(\"Avg Val Discuss Accuracy: {0:.6f}\".format(avg_val_discuss_accuracy))\n",
    "        avg_val_unrelated_accuracy = unrelated_val_true / notrel_val_num\n",
    "        print(\"Avg Val Unrelated Accuracy: {0:.6f}\".format(avg_val_unrelated_accuracy))\n",
    "        print(\"-------------\")\n",
    "\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t1)\n",
    "        \n",
    "        if avg_val_loss < min_val_loss:\n",
    "            min_val_loss = avg_val_loss\n",
    "    \n",
    "        print(\"Avg Validation Loss: {0:.6f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Stance Accur.': avg_val_accuracy_stance,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        model_save_state = {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "    \n",
    "        es.__call__(avg_val_loss, avg_val_accuracy_stance, model_save_state, model_save_path, model, tokenizer)\n",
    "        last_epoch = epoch_i + 1\n",
    "        if es.early_stop == True:\n",
    "            break  # early stop criterion is met, we can stop now\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    \n",
    "    \n",
    "    min_val_loss = es.val_loss_min\n",
    "    max_val_acc = es.val_acc_max_stance\n",
    "\n",
    "    return training_stats, last_epoch, min_val_loss, max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a0faa-4795-4677-be31-f799415cd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(training_stats):\n",
    "    # Display floats with two decimal places.\n",
    "    pd.set_option('precision', 4)\n",
    "    \n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "    # A hack to force the column headers to wrap.\n",
    "    #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "\n",
    "    # Display the table.\n",
    "    print(df_stats)\n",
    "    return df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bd548-dc94-4f5a-8d93-28cb456207a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df_stats, last_epoch):\n",
    "    # Use plot styling from seaborn.\n",
    "    sns.set(style='darkgrid')\n",
    "\n",
    "    # Increase the plot size and font size.\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    \n",
    "    plot1 = plt.figure(1)\n",
    "    \n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training_Loss\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Val_Loss\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    #plt.autoscale(enable=True, axis='x')\n",
    "    \n",
    "    plot2 = plt.figure(2)\n",
    "\n",
    "    x_ticks = []\n",
    "    for currEpoch in range(1, last_epoch+1):\n",
    "        x_ticks.append(currEpoch)\n",
    "    #plt.xticks(x_ticks)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.plot(df_stats['Valid. Stance Accur.'], 'b-o', label=\"Valid. Stance Accur.\")\n",
    "    plt.plot(df_stats['Valid. Ideology Accur.'], 'g-o', label=\"Valid. Ideology Accur.\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Val Stance & Ideology Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Acc\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d26c0c-3f7b-40bb-8185-d9f86e53f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "def run_test_stance_ideology(test_nums, model_savepath, all_input_ids_Test, all_input_masks_Test, stance_labels_Test, ideology_labels_Test, batch_size = 16):       \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #loss_fct = torch.nn.BCELoss()\n",
    "    loss_fct_relatedness = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_, t_test_existedstance, t_test_ideology = preprocess_stance_ideology(stance_labels_Test, ideology_labels_Test)\n",
    "    #t_test_stance = preprocess_ideology_new(stance_labels_Test)\n",
    "\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(all_input_ids_Test, all_input_masks_Test, t_test_relatedness, t_test_stance, t_test_mmd_symbol, t_test_mmd_symbol_, t_test_existedstance, t_test_ideology)\n",
    "    #prediction_data = TensorDataset(all_input_ids_Test, all_input_masks_Test, t_test_stance)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size, num_workers=8, drop_last=True)\n",
    "    \n",
    "    pro_val_num = test_nums[0]\n",
    "    agst_val_num = test_nums[1]\n",
    "    neut_val_num = test_nums[2]\n",
    "    notrel_val_num = test_nums[3]\n",
    "    \n",
    "    total_num = pro_val_num + agst_val_num + neut_val_num + notrel_val_num\n",
    "    \n",
    "    #model_current = 'distilbert-base-uncased'\n",
    "    model_current = 'bert-base-uncased'\n",
    "    #model_current = 'roberta-base'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "        \n",
    "    model = StanceIdeologyDetectionClass(model_current)\n",
    "    checkpoint = torch.load(model_savepath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])    \n",
    "    \n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  betas=(0.9, 0.999), \n",
    "                  eps=1e-08, \n",
    "                  weight_decay=0,\n",
    "                  correct_bias=True\n",
    "    )\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "    optimizer_to(optimizer,device)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    #model.cuda()\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_test_loss = 0.0\n",
    "    \n",
    "    total_test_accuracy_stance = 0.0\n",
    "    total_test_accuracy_ideology = 0.0\n",
    "    ####\n",
    "    agree_test_accuracy = 0.0\n",
    "    disagree_test_accuracy = 0.0\n",
    "    discuss_test_accuracy = 0.0\n",
    "    unrelated_test_accuracy = 0.0\n",
    "    \n",
    "    ideology_test_accuracy = 0.0\n",
    "    ideology_test_con_accuracy = 0.0\n",
    "    ideology_test_lib_accuracy = 0.0\n",
    "    ideology_test_na_accuracy = 0.0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    alpha = 1.5\n",
    "    theta = 0\n",
    "    beta = 1e-3\n",
    "    gamma = 0\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "      #Add batch to GPU\n",
    "        \n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_relatedness = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "        b_mmd_symbol = batch[4].to(device)\n",
    "        b_mmd_symbol_ = batch[5].to(device)\n",
    "        b_existedstances = batch[6].to(device)\n",
    "        b_ideologies = batch[7].to(device)\n",
    "  \n",
    "\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():         \n",
    "            # Forward pass, calculate logit predictions\n",
    "            \n",
    "            n1 = torch.sum(b_mmd_symbol, dim=0)\n",
    "            n2 = torch.sum(b_mmd_symbol_, dim=0)\n",
    "        \n",
    "            aa = torch.reshape(b_mmd_symbol, (-1,1))\n",
    "            bb = torch.reshape(b_mmd_symbol_, (-1,1))\n",
    "            \n",
    "            mmd_loss, P_relatedness, P_stance, P_oneside, P_ideology = model(input_ids = b_input_ids, attention_mask = b_input_mask, mmd_pl = b_mmd_symbol, mmd_pl_ = b_mmd_symbol_)\n",
    "            #P_stance = model(input_ids = b_input_ids, attention_mask = b_input_mask)\n",
    "                \n",
    "            \n",
    "            relatedness_loss = loss_fct_relatedness(P_relatedness, b_relatedness.float())\n",
    "            #relatedness_loss = 0\n",
    "            stance_loss = loss_fct_relatedness(P_stance, b_labels.float())\n",
    "            existedstance_loss = loss_fct_relatedness(P_oneside, b_existedstances.float())\n",
    "            ideology_loss = loss_fct_relatedness(P_ideology, b_ideologies.float())\n",
    "                \n",
    "    \n",
    "            loss_test = relatedness_loss + alpha * stance_loss + beta * mmd_loss\n",
    "            #loss_test = alpha * stance_loss\n",
    "            total_test_loss += loss_test.item()\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            P_relatedness = P_relatedness.to('cpu')\n",
    "            b_relatedness = b_relatedness.to('cpu')\n",
    "            P_stance = P_stance.to('cpu')\n",
    "            b_labels = b_labels.to('cpu')\n",
    "            b_existedstances = b_existedstances.to('cpu')\n",
    "            P_oneside = P_oneside.to('cpu')\n",
    "            P_ideology = P_ideology.to('cpu')\n",
    "            b_ideologies = b_ideologies.to('cpu')\n",
    "            \n",
    "            acc_list = predict_classwise_stance_ideology(P_relatedness, P_stance, b_labels, P_ideology, b_ideologies)\n",
    "            total_test_accuracy_stance += acc_list[0]\n",
    "            ###\n",
    "            agree_test_accuracy += acc_list[1]\n",
    "            disagree_test_accuracy += acc_list[2]\n",
    "            discuss_test_accuracy += acc_list[3]\n",
    "            unrelated_test_accuracy += acc_list[4]\n",
    "            total_test_accuracy_ideology += acc_list[6]\n",
    "            #\n",
    "            ideology_test_con_accuracy += acc_list[7]\n",
    "            ideology_test_lib_accuracy += acc_list[8]\n",
    "            ideology_test_na_accuracy += acc_list[9]    \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_test_loss = total_test_loss / len(prediction_dataloader)\n",
    "    avg_test_accuracy_stance = total_test_accuracy_stance\n",
    "    avg_test_accuracy_ideology = total_test_accuracy_ideology / len(prediction_dataloader)\n",
    "    \n",
    "    avg_agree_test_acc = agree_test_accuracy / pro_val_num\n",
    "    avg_disagree_test_acc = disagree_test_accuracy / agst_val_num\n",
    "    avg_discuss_test_acc = discuss_test_accuracy / neut_val_num\n",
    "    avg_unrelated_test_acc = unrelated_test_accuracy / notrel_val_num\n",
    "    \n",
    "    avg_con_test_acc = ideology_test_con_accuracy / len(prediction_dataloader)\n",
    "    avg_lib_test_acc = ideology_test_lib_accuracy / len(prediction_dataloader)\n",
    "    avg_na_test_acc = ideology_test_na_accuracy / len(prediction_dataloader)\n",
    "\n",
    "    return avg_test_loss, avg_test_accuracy_stance, avg_agree_test_acc, avg_disagree_test_acc, avg_discuss_test_acc, avg_unrelated_test_acc, avg_test_accuracy_ideology, avg_con_test_acc, avg_lib_test_acc, avg_na_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d9d86-1b3d-45ea-a034-c322413adc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_we(df, model):\n",
    "    my_set = []\n",
    "    for my_doc in df:\n",
    "        claim = instance['Q']      \n",
    "        headline = instance['title'] + \" \" + instance['docCont']\n",
    "        full_str = clain + headline\n",
    "        feat_vec = model.get_sentence_vector(full_str)\n",
    "        my_set.append(feat_vec)\n",
    "    return my_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf38de0-28f4-4fe1-b29b-1c311f66f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wholeprocess_stance_serp_prev(train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate, seedVal):\n",
    "    device = run_utils()\n",
    "    model_save_path = './model_save/fnc/model_fncbert.t7'  \n",
    "    #import fasttext.util\n",
    "    #fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "    #ft = fasttext.load_model('cc.en.300.bin')\n",
    "    \n",
    "    #model_fnc = './model_save/fnc/model_fnc.t7'    \n",
    "\n",
    "#--------------LOAD DATASETS--------------#\n",
    "\n",
    "    model_current = 'bert-base-uncased'\n",
    "    tokenizer = load_tokenizer(model_current)\n",
    "    \n",
    "    #model_current = fasttext.load_model('/notebooks/fastText/model.bin')\n",
    "\n",
    "    lim_unigram = 3000\n",
    "\n",
    "    df = load_dataset_stance('./dataset/batches_cleaned/stance/train_serp.tsv')\n",
    "    dfVal = load_dataset_stance('./dataset/batches_cleaned/stance/val_serp.tsv')\n",
    "    dfTest = load_dataset_stance('./dataset/batches_cleaned/stance/test_serp.tsv')\n",
    "    \n",
    "    train_nums = count_class_num (df)\n",
    "    val_nums = count_class_num (dfVal)\n",
    "    test_nums = count_class_num (dfTest)\n",
    "\n",
    "    \n",
    "    train_set, train_mean, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = preprocess_data_train(df, dfVal, dfTest, lim_unigram)\n",
    "    val_set = preprocess_data_val(dfVal, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m = 0.0001)\n",
    "    test_set = preprocess_data_test(dfTest, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m = 0.0001)\n",
    "    \n",
    "        \n",
    "    # Report the number of sentences.\n",
    "    print('Number of training sentences: {:,}'.format(df.shape[0]))\n",
    "    print('Number of val sentences: {:,}'.format(dfVal.shape[0]))\n",
    "    print('Number of test sentences: {:,}'.format(dfTest.shape[0]))\n",
    "\n",
    "\n",
    "    labelsTrain = df['stance']\n",
    "    labelsVal = dfVal['stance']\n",
    "    labelsTest = dfTest['stance']\n",
    "\n",
    "    \n",
    "    #--------------TRAINING-------------#\n",
    "    \n",
    "    model, train_dataloader, validation_dataloader, optimizer, scheduler = prepare_for_training_stance_ideology(train_set, labelsTrain, val_set, labelsVal, model_current, batch_size, epochs, \n",
    "                                                                                                                num_warmup_steps, learning_rate)\n",
    "    training_stats, last_epoch, min_val_loss, max_val_acc = train_stance_ideology (train_nums, val_nums, model_save_path, model, tokenizer, train_dataloader, validation_dataloader, epochs, batch_size, optimizer,\n",
    "                                                                          scheduler, patience, verbose, delta, seedVal, False)\n",
    "\n",
    "        \n",
    "    df_stats = print_summary(training_stats)\n",
    "    plot_results(df_stats, last_epoch)\n",
    "\n",
    "    print('Min Val Loss: ' + str(min_val_loss))\n",
    "    print('Max Val Acc: ' + str(max_val_acc))\n",
    "    \n",
    "    \n",
    "    test_loss, test_acc, avg_agree_test_acc, avg_disagree_test_acc, avg_discuss_test_acc, avg_unrelated_test_acc, test_acc_ideology, test_acc_con_id, test_acc_lib_id, test_acc_na_id = run_test_stance_ideology(test_nums, model_save_path, all_input_ids_Test, all_input_masks_Test, labelsTest, labelsTest_ideology)\n",
    "    \n",
    "    print('Test Loss: ' + str(test_loss))\n",
    "    print('Test Stance Acc: ' + str(test_acc))\n",
    "    print('Test Ideology Acc: ' + str(test_acc_ideology))\n",
    "    \n",
    "    print('Agree Class Acc: ' + str(avg_agree_test_acc))\n",
    "    print('Disagree Class Acc: ' + str(avg_disagree_test_acc))\n",
    "    print('Discuss Class Acc: ' + str(avg_discuss_test_acc))\n",
    "    print('Unrelated Class Acc: ' + str(avg_unrelated_test_acc))\n",
    "    \n",
    "    print('Con Class Acc: ' + str(test_acc_con_id))\n",
    "    print('Lib Class Acc: ' + str(test_acc_lib_id))\n",
    "    print('NA Class Acc: ' + str(test_acc_na_id))\n",
    "    \n",
    "    #model_to_save.save_pretrained('model_save')\n",
    "    #tokenizer.save_pretrained('model_save')\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    #torch.save(args, os.path.join('model_save', 'training_args.bin'))\n",
    "    #model_args = str(max_len) + '_' + str(doc_stride) + '_' + str(batch_size) + \"_\" + str(learning_rate) + \"_warmup\" + str(num_warmup_steps) + \"_seedVal\" + str(seedVal)\n",
    "    #model_path = model_save_path + '/model_' + model_args\n",
    "    #model.save_pretrained(model_save_path)\n",
    "    #torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d2d80-39a0-49d3-a8a7-1f13d42df51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "def run_wholeprocess_stance_serp(train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate, seedVal):\n",
    "    device = run_utils()\n",
    "    #model_save_path = './model_save/fnc/model_emergentbert_epoch90_withoutsep_serp.t7'  \n",
    "     \n",
    "    #model_fnc = './model_save/fnc/model_fnc.t7'   \n",
    "    #model_current = \"ponmari/Question-Answering\"\n",
    "        \n",
    "    #model_current = 'distilbert-base-uncased'\n",
    "    #model_current = './models/BERT_SERP/bert_titleonly_finetuned'\n",
    "    #model_current = 'roberta-large'\n",
    "    #model_current = 'bert-base-uncased'\n",
    "    #model_current = 'xlnet-base-cased'\n",
    "    #model_current = 'bert-base-multilingual-cased'\n",
    "    #model_current = 'albert-base-v2'\n",
    "    #model_current = './models/mnli_model/'\n",
    "    #model_current = './model_save/fnc/model_emergentbert_epoch90.t7'\n",
    "    \n",
    "\n",
    "#--------------LOAD DATASETS--------------#\n",
    "\n",
    "    train_path = 'emergent_train.csv'\n",
    "    val_path = 'emergent_val.csv'\n",
    "    test_path = 'emergent_test.csv'\n",
    "    \n",
    "    ##df = load_dataset_emergent(train_path)\n",
    "    #dfVal = load_dataset_emergent(val_path)\n",
    "    #dfTest = load_dataset_emergent(test_path)\n",
    "    \n",
    "    #df = pd.concat([df, dfVal], ignore_index=True)\n",
    "    \n",
    "\n",
    "    #train_path = './dataset/batches_cleaned/stance/Train_latest.tsv'\n",
    "    #val_path = './dataset/batches_cleaned/stance/Val_latest.tsv'\n",
    "    #test_path = './dataset/batches_cleaned/stance/Test_latest.tsv'\n",
    "    \n",
    "    labelsTrain_ideology = []\n",
    "    labelsVal_ideology = []\n",
    "    labelsTest_ideology = []\n",
    "    \n",
    "    train_nums_ideology = 0\n",
    "    val_nums_ideology = 0\n",
    "    test_nums_ideology = 0\n",
    "    \n",
    "    lim_unigram = 3000\n",
    "\n",
    "    trainPer = 1.2\n",
    "    valPer = 0.2\n",
    "    testPer = 0.2\n",
    "    \n",
    "    df_all = load_dataset_stance('./dataset/batches_cleaned/stance/MergedDataset_20.06.2021.tsv')\n",
    "    create_more_notrel_docs(df_all)\n",
    "    \n",
    "    df_all_added = load_dataset_stance('./dataset/batches_cleaned/stance/Final_Dataset_AddedNotRelated.tsv')\n",
    "    y = df_all_added['stance'].copy(deep=True)\n",
    "    X = df_all_added.drop('stance', axis=1).copy(deep=True)\n",
    "    \n",
    "    print(seedVal)\n",
    "    #split train and test datasets once\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, random_state=seedVal, stratify = y)\n",
    "    \n",
    "    \n",
    "        \n",
    "    train_nums = count_class_num (df_all)\n",
    "    # Report the number of sentences.\n",
    "    print('Number of all sentences: ', train_nums)\n",
    "    \n",
    "    dfTrain = X_train.copy(deep=True)\n",
    "    dfTest = X_test.copy(deep=True)\n",
    "    \n",
    "    dfTrain.insert(2, \"stance\", y_train.values)\n",
    "    dfTest.insert(2, \"stance\", y_test.values) \n",
    "    \n",
    "    dfTrain.to_csv('./dataset/batches_cleaned/stance/train_serp.tsv', sep='\\t', index=False)\n",
    "    dfTest.to_csv('./dataset/batches_cleaned/stance/test_serp.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    test_nums = count_class_num (dfTest)\n",
    "    print('Number of test sentences: {:,}'.format(dfTest.shape[0]))\n",
    "    \n",
    "    #X_train.to_csv('./dataset/batches_cleaned/stance/train_serp.tsv', sep='\\t', index=False)\n",
    "    #X_val.to_csv('./dataset/batches_cleaned/stance/val_serp.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    k_fold = 5\n",
    "    model_list = ['bert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n",
    "    \n",
    "    \n",
    "    num_base_models = len(model_list)\n",
    "    model_save_list = []\n",
    "    model_losses = []\n",
    "    all_val_folds_all_predictions = np.empty((100, num_base_models*4))\n",
    "    all_val_encoded_targets = np.empty((100, 4))\n",
    "    best_val_losses = []\n",
    "    best_model_savepaths = []\n",
    "    for idx in range(0, k_fold):\n",
    "        X = pd.DataFrame (columns=['qID', 'docID', 'ideology', 'docCont' 'Q', 'title'])\n",
    "        y = pd.DataFrame (columns=['stance'])\n",
    "        \n",
    "        X_2 = pd.DataFrame (columns=['qID', 'docID', 'ideology', 'docCont' 'Q', 'title'])\n",
    "        y_2 = pd.DataFrame (columns=['stance'])\n",
    "        \n",
    "        print(\"******************\")\n",
    "        print(\"Training is starting for cross validation dataset \" + str(idx))\n",
    "        print(\"******************\")\n",
    "        \n",
    "        seedVal = get_random_seed()\n",
    "        print(seedVal)\n",
    "        \n",
    "        #creating a k-fold CV\n",
    "        X, X_2, y, y_2 = train_test_split(X_train, y_train, test_size=1.0/k_fold, shuffle = True, random_state = seedVal, stratify = y_train)\n",
    "        \n",
    "        df = X.copy(deep=True)\n",
    "        dfVal = X_2.copy(deep=True)\n",
    "        \n",
    "        df.insert(2, \"stance\", y.values)\n",
    "        dfVal.insert(2, \"stance\", y_2.values)\n",
    "        \n",
    "        df.to_csv('./dataset/batches_cleaned/stance/train_serp' + \"_\" + str(idx) + \".tsv\", sep='\\t', index=False)\n",
    "        dfVal.to_csv('./dataset/batches_cleaned/stance/val_serp' + \"_\" + str(idx) + \".tsv\", sep='\\t', index=False)\n",
    "        \n",
    "        #df = pd.read_csv('./dataset/batches_cleaned/stance/train_serp' + \"_\" + str(idx) + \".tsv\", sep='\\t')\n",
    "        #dfVal = pd.read_csv('./dataset/batches_cleaned/stance/val_serp' + \"_\" + str(idx) + \".tsv\", sep='\\t')\n",
    "        \n",
    "        #df = pd.read_csv(path, delimiter='\\t', header = 0, names=['qID', 'docID', 'stance', 'ideology', 'docCont', 'Q', 'title']\n",
    "        \n",
    "        \n",
    "    \n",
    "        train_nums = count_class_num (df)\n",
    "        val_nums = count_class_num (dfVal)\n",
    "\n",
    "    \n",
    "        #sentencesQueryCont_Train = []\n",
    "        labelsTrain = df['stance']\n",
    "        labelsVal = dfVal['stance']\n",
    "        \n",
    "        m = 0.0001\n",
    "\n",
    "        #sentencesQueryTitle_Train, labelsTrain = generate_datasets_emergent (df)\n",
    "        train_set, train_mean, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = preprocess_data_train(df, dfVal, dfTest, lim_unigram, idx+1)\n",
    "        val_set = preprocess_data_val(dfVal, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m, idx+1)\n",
    "        \n",
    "        #sentencesQueryTitle_Train, sentencesQueryTitleCont_Train, labelsTrain, labelsTrain_ideology = generate_datasets (df)\n",
    "\n",
    "\n",
    "        #sentencesQueryCont_Val = []\n",
    "\n",
    "        #--------------DATASETS-------------#\n",
    "    \n",
    "        #sentencesQueryTitle_Val, labelsVal = generate_datasets_emergent (dfVal)\n",
    "        #sentencesQueryTitle_Val, sentencesQueryTitleCont_Val, labelsVal, labelsVal_ideology = generate_datasets (dfVal)\n",
    "        \n",
    "        encoded_current_val_targets = preprocess_stance_ideology_meta(labelsVal)\n",
    "        if idx == 0:\n",
    "            all_val_encoded_targets = encoded_current_val_targets\n",
    "        else:\n",
    "            all_val_encoded_targets = np.concatenate((all_val_encoded_targets, encoded_current_val_targets))\n",
    "    \n",
    "        # Report the number of sentences.\n",
    "        print('Number of training sentences: ', train_nums)\n",
    "        print('Number of val sentences: ', val_nums)\n",
    "        \n",
    "        #first: bert-base, second: roberta-base, third: xlnet-base\n",
    "        \n",
    "        current_val_fold_all_predictions = np.empty((dfVal.shape[0], num_base_models))\n",
    "        \n",
    "        for idxModel in range(0, num_base_models):\n",
    "            \n",
    "            model_current = model_list[idxModel]\n",
    "            tokenizer = load_tokenizer(model_current)\n",
    "            \n",
    "            #tokenize the dataset for current k-fold training and val\n",
    "            \n",
    "            #all_input_ids_Train, all_input_masks_Train  = preprocessing_for_bert(tokenizer, sentencesQueryTitleCont_Train, max_len, doc_stride) #train\n",
    "            #all_input_ids_Val, all_input_masks_Val = preprocessing_for_bert(tokenizer, sentencesQueryTitleCont_Val, max_len, doc_stride) #train\n",
    "\n",
    "            #--------------TRAINING-------------#\n",
    "        \n",
    "            model_save_path = './models/BERT_SERP/model_finetuned' + \"_\" + str(idxModel) + \"_\" + str(idx)\n",
    "            model, train_dataloader, validation_dataloader, optimizer, scheduler = prepare_for_training_stance_ideology(train_set, labelsTrain, val_set, labelsVal, model_current, batch_size, epochs, \n",
    "                                                                                                                num_warmup_steps, learning_rate)\n",
    "            training_stats, last_epoch, min_val_loss, max_val_acc = train_stance_ideology (train_nums, val_nums, model_save_path, model, tokenizer, train_dataloader, validation_dataloader, epochs, batch_size, optimizer,\n",
    "                                                                          scheduler, patience, verbose, delta, seedVal, False)\n",
    "        \n",
    "        \n",
    "            val_loss, val_acc, avg_agree_val_acc, avg_disagree_val_acc, avg_discuss_val_acc, avg_unrelated_val_acc, val_predictions = run_test_stance_ideology(val_nums, val_nums_ideology, model_current, model_save_path, all_input_ids_Val, all_input_masks_Val, labelsVal, labelsVal_ideology)\n",
    "            #if idx == 0:\n",
    "                #best_val_losses.append(val_loss)\n",
    "                #best_model_savepaths.append(model_save_path)\n",
    "            #else:\n",
    "                #if val_loss < best_val_losses[idxModel]:\n",
    "                    #best_val_losses[idxModel] = val_loss\n",
    "                    #best_model_savepaths[idxModel] = model_save_path\n",
    "            \n",
    "            print(len(val_predictions))\n",
    "            if idxModel == 0:\n",
    "                current_val_fold_all_predictions = val_predictions\n",
    "            else:\n",
    "                current_val_fold_all_predictions = np.concatenate((current_val_fold_all_predictions, val_predictions), axis=1)\n",
    "                \n",
    "            print(current_val_fold_all_predictions.shape)\n",
    "\n",
    "        print(\"**************\")\n",
    "        print(current_val_fold_all_predictions)\n",
    "        \n",
    "        filename_fold_predictions = './dataset/batches_cleaned/stance/cv_set_val_predictions_' + str(idx) + '.tsv'\n",
    "        pd.DataFrame(current_val_fold_all_predictions).to_csv(filename_fold_predictions, sep='\\t', header=None, index=None)\n",
    "        \n",
    "        filename_fold_targets = './dataset/batches_cleaned/stance/cv_set_val_targets_' + str(idx) + '.tsv'\n",
    "        pd.DataFrame(encoded_current_val_targets).to_csv(filename_fold_targets, sep='\\t', header=None, index=None)\n",
    "    \n",
    "        if idx == 0:\n",
    "            all_val_folds_all_predictions = current_val_fold_all_predictions\n",
    "        else:\n",
    "            all_val_folds_all_predictions = np.concatenate((all_val_folds_all_predictions,current_val_fold_all_predictions))\n",
    "    \n",
    "    #all_val_encoded_targets_tensor = torch.cat(all_val_encoded_targets, dim=0)\n",
    "    #meta-learner phase\n",
    "    \n",
    "    current_val_fold_all_predictions = np.empty((100, num_base_models))\n",
    "    encoded_current_val_targets = np.empty((100, k_fold))\n",
    "    for idx in range(0, k_fold):\n",
    "        filename_curr = './dataset/batches_cleaned/stance/cv_set_val_predictions_' + str(idx) + '.tsv'\n",
    "        current_val_fold_all_predictions = pd.read_csv(filename_curr, sep='\\t', header = 0, dtype='float64')\n",
    "        \n",
    "        \n",
    "        filename_fold_targets = './dataset/batches_cleaned/stance/cv_set_val_targets_' + str(idx) + '.tsv'\n",
    "        encoded_current_val_targets = pd.read_csv(filename_fold_targets, sep='\\t', header = 0, dtype='int64')\n",
    "        #current_val_fold_all_predictions = np.loadtxt(filename_curr, dtype=float)\n",
    "        \n",
    "        if idx == 0:\n",
    "            all_val_folds_all_predictions = current_val_fold_all_predictions\n",
    "            all_val_encoded_targets = encoded_current_val_targets\n",
    "        else:\n",
    "            all_val_folds_all_predictions = np.concatenate((all_val_folds_all_predictions, current_val_fold_all_predictions))\n",
    "            all_val_encoded_targets = np.concatenate((all_val_encoded_targets, encoded_current_val_targets))\n",
    "    \n",
    "    \n",
    "    for idxModel in range(0, num_base_models):\n",
    "        model_save_path = './models/BERT_SERP/model_finetuned' + \"_\" + str(idxModel) + \"_\" + str(0)\n",
    "        best_model_savepaths.append(model_save_path)\n",
    "\n",
    "    print(\"---------------\")\n",
    "    print(all_val_folds_all_predictions.shape)\n",
    "    print(all_val_encoded_targets.shape)\n",
    "    print(\"Meta Learning Training is Starting...\")\n",
    "    \n",
    "    X = pd.DataFrame (columns=['qID', 'docID', 'ideology', 'docCont' 'Q', 'title'])\n",
    "    y = pd.DataFrame (columns=['pro', 'agst', 'neut', 'not-rel'])\n",
    "        \n",
    "    X_val = pd.DataFrame (columns=['qID', 'docID', 'ideology', 'docCont' 'Q', 'title'])\n",
    "    y_val = pd.DataFrame (columns=['pro', 'agst', 'neut', 'not-rel'])\n",
    "    \n",
    "    new_df = pd.DataFrame(all_val_folds_all_predictions)\n",
    "    new_y = pd.DataFrame(all_val_encoded_targets)\n",
    "    \n",
    "    #creating a k-fold CV\n",
    "    X, X_val, y, y_val = train_test_split(new_df, new_y, test_size=0.25, shuffle = True, random_state = seedVal, stratify = new_y)\n",
    "    \n",
    "    df = X.copy()\n",
    "    dfVal = X_val.copy()\n",
    "\n",
    "    \n",
    "    #There is a problem here!\n",
    "    train_nums = count_class_num (y)\n",
    "    val_nums = count_class_num (y_val)\n",
    "    \n",
    "    print(train_nums)\n",
    "    print(val_nums)\n",
    "    \n",
    "    model_save_path_metalearner = './models/BERT_SERP/metalearner'\n",
    "    \n",
    "    \n",
    "    model, train_dataloader, validation_dataloader, optimizer, scheduler = prepare_for_training_stance_ideology_metalearner(X, y, X_val, y_val, batch_size, epochs, num_warmup_steps, learning_rate)\n",
    "    training_stats, last_epoch, min_val_loss, max_val_acc = train_stance_ideology_metalearner (train_nums, val_nums, train_nums_ideology, val_nums_ideology, model_save_path_metalearner, model, train_dataloader, validation_dataloader, epochs, batch_size, optimizer,\n",
    "                                                                            scheduler, patience, verbose, delta, seedVal, False)    \n",
    "    \n",
    "    \n",
    "    print(\"Testing phase is starting...\")\n",
    "    #apply the same process to the test set\n",
    "    \n",
    "    sentencesQueryCont_Test = []\n",
    "    labelsTest = dfTest['stance']\n",
    "    \n",
    "    #sentencesQueryTitle_Test, labelsTest = generate_datasets_emergent (dfTest)\n",
    "    #sentencesQueryTitle_Test, sentencesQueryTitleCont_Test, labelsTest, labelsTest_ideology = generate_datasets (dfTest)\n",
    "    test_set = preprocess_data_test(dfTest, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer, m)\n",
    "    \n",
    "    current_test_fold_all_predictions = np.empty((dfTest.shape[0], num_base_models))\n",
    "    for idxModel in range(0, num_base_models):\n",
    "        model_current = model_list[idxModel]\n",
    "        tokenizer = load_tokenizer(model_current)\n",
    "        \n",
    "        #all_input_ids_Test, all_input_masks_Test = preprocessing_for_bert(tokenizer, sentencesQueryTitleCont_Test, max_len, doc_stride) #test\n",
    "        \n",
    "        model_save_path = './models/BERT_SERP/model_finetuned' + \"_\" + str(idxModel) + \"_\" + str(4)\n",
    "        \n",
    "        print(\"Current best model: \" + str(model_save_path))\n",
    "        \n",
    "        test_loss, test_acc, avg_agree_test_acc, avg_disagree_test_acc, avg_discuss_test_acc, avg_unrelated_test_acc, test_predictions = run_test_stance_ideology(test_nums, test_nums_ideology, model_current, \n",
    "        model_save_path, all_input_ids_Test, all_input_masks_Test, labelsTest, labelsTest_ideology)\n",
    "        \n",
    "        if idxModel == 0:\n",
    "            current_test_fold_all_predictions = test_predictions\n",
    "        else:\n",
    "            current_test_fold_all_predictions = np.concatenate((current_test_fold_all_predictions, test_predictions), axis=1)\n",
    "            \n",
    "    print(current_test_fold_all_predictions.shape)\n",
    "    print(labelsTest.shape)\n",
    "    \n",
    "    print(\"Testing Meta phase is starting...\")\n",
    " \n",
    "    test_loss, test_acc, avg_agree_test_acc, avg_disagree_test_acc, avg_discuss_test_acc, avg_unrelated_test_acc = run_test_stance_ideology_meta(test_nums, test_nums_ideology, model_current, model_save_path_metalearner, current_test_fold_all_predictions, labelsTest, labelsTest_ideology)\n",
    "    \n",
    "    print(\"****************\")\n",
    "    print('Test Loss: ' + str(test_loss))\n",
    "    print('Test Stance Acc: ' + str(test_acc))\n",
    "    \n",
    "    print('Agree Class Acc: ' + str(avg_agree_test_acc))\n",
    "    print('Disagree Class Acc: ' + str(avg_disagree_test_acc))\n",
    "    print('Discuss Class Acc: ' + str(avg_discuss_test_acc))\n",
    "    print('Unrelated Class Acc: ' + str(avg_unrelated_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0092e5e-e1bc-4cf9-915e-9835133ea947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_determinism(seedVal):\n",
    "    torch.manual_seed(seedVal)\n",
    "    torch.cuda.manual_seed(seedVal)\n",
    "    np.random.seed(seedVal)\n",
    "    random.seed(seedVal)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dacaf-c9cc-4d1e-b476-cace073088c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%rm -rf \"./runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983fb995-3acb-4480-b12d-d6807fd6fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install import_ipynb\n",
    "#!pip install nltk\n",
    "#import nltk\n",
    "#nltk.download('popular')\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390cc36-60ea-424b-838a-76b4ce8adda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "import time\n",
    "import datetime\n",
    "from transformers import AutoModel\n",
    "from transformers import DistilBertModel\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import import_ipynb\n",
    "import feature_engineering\n",
    "from feature_engineering import *\n",
    "from Utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import module_mix\n",
    "from module_mix import MixLinear\n",
    "\n",
    "#model = \"bert-base-uncased\"\n",
    "train_path = './dataset/fnc/train'\n",
    "val_path = './dataset/fnc/test'\n",
    "\n",
    "model_save_path = './model_save'\n",
    "\n",
    "\n",
    "max_len = 512\n",
    "doc_stride = 0\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 300\n",
    "num_warmup_steps = 10\n",
    "learning_rate = 5e-6\n",
    "##-----Early Stopping\n",
    "patience = 10\n",
    "verbose = True\n",
    "delta = 0.000001\n",
    "\n",
    "seedVal = get_random_seed()\n",
    "print(seedVal)\n",
    "\n",
    "\n",
    "#seedVal = 40\n",
    "\n",
    "#0 - tinybert\n",
    "#1 - distilbert\n",
    "#2 - bert\n",
    "\n",
    "#bert_type = 2\n",
    "\n",
    "#create_determinism(seedVal)\n",
    "\n",
    "#folder_preparations()\n",
    "run_wholeprocess_stance_serp_prev(train_path, val_path, max_len, doc_stride, batch_size, num_warmup_steps, learning_rate, seedVal)\n",
    "#create_more_notrel_docs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
